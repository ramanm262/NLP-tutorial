{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ef21c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements: scikit-learn, nltk, gensim, tensorflow\n",
    "# ! pip install scikit-learn\n",
    "# ! pip install nltk\n",
    "# ! pip install gensim\n",
    "# ! pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4f126",
   "metadata": {},
   "source": [
    "# A Super High-Level Tutorial on Some Basics of Natural Language Processing\n",
    "\n",
    "No, despite this NLP tutorial involving neural networks, language, and teaching you programming, NLP does not stand for neuro-linguistic programming here. (However, it could stand for that if you're willing to step into such a future...)\n",
    "\n",
    "#### What is meant by \"natural language\"?\n",
    "Well, it's something with a pretty good Wikipedia article: https://en.wikipedia.org/wiki/Natural_language\n",
    "Importantly, it's a language that you can understand \"without conscious planning or premeditation\". This is in contrast to a formal language, which is the kind of language you'd expect a computer to be able to understand.\n",
    "\n",
    "NLP is all about using formal languages to create representations of natural languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b0c109",
   "metadata": {},
   "source": [
    "#### NLP Jargon Dictionary\n",
    "\n",
    "This is definitely not comprehensive, but it covers all of the jargon in this notebook.\n",
    "\n",
    "- **Token** >> The smallest unit of natural-language information available to a machine. For example, the strings \" wildcat \" or \" 's \". You might have \"I like wildcats\" as a token, but you should probably split it up into three tokens.\n",
    "- **Document.** >> A single collection of tokens that go together. A sentence might be a document, or if you're classifying poems by author, the entirety of T. S. Eliot's *The Waste Land* might be a document in your training set.\n",
    "- **Corpus.** >> Your entire collection of tokens and/or documents. In the previous example, all your poems would constitute your corpus, before you do a train/validation/test split.\n",
    "- **Embedding** >> The representation of natural-language information in vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeebaf3",
   "metadata": {},
   "source": [
    "#### How the heck do you even quantify language?\n",
    "\n",
    "A naive -- but essential -- way to turn linguistic information into mathematical objects is the \"bag of words\" approach, which assumes that your tokens are fundamental units of meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac149167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A monk said to Seigen What is the essence of Buddhism Seigen said What is the price of rice in Roryo\n"
     ]
    }
   ],
   "source": [
    "# Start with some text that is super easy for humans to parse...but maybe not to understand ㋡\n",
    "koan = \"A monk said to Seigen, “What is the essence of Buddhism?” Seigen said, “What is the price of rice in Roryo?”\"\n",
    "\n",
    "# Remove punctuation. This does throw away some linguistic information, but it's easier to not have to deal with it now.\n",
    "punctuation_dict = [',', '“', '?', '”']\n",
    "for char in punctuation_dict:\n",
    "    koan = koan.replace(char, '')\n",
    "print(koan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aac8344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'monk', 'said', 'to', 'Seigen', 'What', 'is', 'the', 'essence', 'of', 'Buddhism', 'Seigen', 'said', 'What', 'is', 'the', 'price', 'of', 'rice', 'in', 'Roryo']\n",
      "Number of words: 21\n"
     ]
    }
   ],
   "source": [
    "# Split up the koan so the computer can recognize the words as distinct objects\n",
    "split_koan = koan.split(' ')\n",
    "print(split_koan)\n",
    "print(f\"Number of words: {len(split_koan)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6216cbaf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['buddhism' 'essence' 'in' 'is' 'monk' 'of' 'price' 'rice' 'roryo' 'said'\n",
      " 'seigen' 'the' 'to' 'what'] \n",
      "\n",
      "Vocab size:  (14,) \n",
      "\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X = count_vect.fit_transform(split_koan)\n",
    "print(\"Vocabulary: \", count_vect.get_feature_names_out(),\"\\n\")\n",
    "print(\"Vocab size: \", count_vect.get_feature_names_out().shape, \"\\n\")\n",
    "print(X.toarray())\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42369753",
   "metadata": {},
   "source": [
    "This is now a (very sparse) list of features you can train a model on, for example, to predict what word comes next! But we'll get to that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c25d041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It says 'Seigen' at position(s): [4, 11]\n",
      "Here are the corresponding rows in the token matrix:\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "word_of_interest = \"Seigen\"\n",
    "where_does_it_say = [idx for idx in range(len(split_koan)) if split_koan[idx] == word_of_interest]\n",
    "print(f\"It says '{word_of_interest}' at position(s): {where_does_it_say}\")\n",
    "print(\"Here are the corresponding rows in the token matrix:\")\n",
    "for instance in where_does_it_say: # Haha, \"for instance\"\n",
    "    print(X.toarray()[instance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a037da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buddhism' 'essence' 'in' 'is' 'monk' 'of' 'price' 'rice' 'roryo' 'said'\n",
      " 'seigen' 'the' 'to' 'what']\n",
      "[1 1 1 2 1 2 1 1 1 2 2 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "# You can get the frequency of each token by taking the sum over the 0 axis\n",
    "print(count_vect.get_feature_names_out())\n",
    "print(X.toarray().sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a9fb06",
   "metadata": {},
   "source": [
    "## Let's complicate things a bit.\n",
    "\n",
    "Bag of words is super easy to do, but the representation it builds of a document is very high-dimensional and very sparse. This isn't good for memory or computation time, so we'd have to get creative. Good thing some people are paid to be creative so we don't have to be.\n",
    "\n",
    "#### Using Word2Vec\n",
    "Word2Vec is an algorithm that 1) creates vector representations of tokens as we did above (that is, embeddings) and 2) uses a neural network to either predict a word given its context or, given a word, to predict its context. I won't go into more detail than that, but the original paper is here, and the whole thing is very searchable since it was so widely used: https://arxiv.org/pdf/1301.3781.pdf\n",
    "\n",
    "(Side note: One of the corresponding authors' emails is \"jeff@google.com\", an account name which I covet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "964e0ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\Raman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"state_union\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78b45052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['PRESIDENT',\n",
       "  'HARRY',\n",
       "  'S',\n",
       "  '.',\n",
       "  'TRUMAN',\n",
       "  \"'\",\n",
       "  'S',\n",
       "  'ADDRESS',\n",
       "  'BEFORE',\n",
       "  'A',\n",
       "  'JOINT',\n",
       "  'SESSION',\n",
       "  'OF',\n",
       "  'THE',\n",
       "  'CONGRESS'],\n",
       " ['April', '16', ',', '1945']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import state_union\n",
    "sotu = list(state_union.sents())\n",
    "sotu[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7a727e",
   "metadata": {},
   "source": [
    "Already we can see an approaching problem. The dataset has already been split up by word, but we still have to remove a ton of punctuation, and near-useless tokens like the possessive \"S\". Also, words like \"A\" don't contribute much to meaning, but they do take up space in our high-dimensional embeddings, when that space could be used to better represent more meaning from other tokens. So, we also have to remove \"A\" and other similar \"stopwords\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75cf7738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Raman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7edfbc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning token list: 100%|██████████| 17930/17930 [00:00<00:00, 125363.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['president', 'harry', 'truman', 'address', 'joint', 'session', 'congress'], ['april', '16', '1945']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "punctuation = {',', '.', \"'\", '\"', ':', ';', '!', '?', '-', '--'}\n",
    "\n",
    "cleaned_corpus = []\n",
    "for sentence in tqdm.tqdm(sotu, desc=\"Cleaning token list\"):\n",
    "    cleaned_sentence = []\n",
    "    for token in sentence:\n",
    "        token = token.lower() # Because Harry Truman didn't yell much\n",
    "        if (token not in stop_words) and (token not in punctuation):\n",
    "            cleaned_sentence.append(token)\n",
    "    cleaned_corpus.append(cleaned_sentence)\n",
    "print(cleaned_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e671bd7",
   "metadata": {},
   "source": [
    "Let's take a look at the most frequent tokens, now that the stopwords have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bee70ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent tokens:\n",
      "['world', 'year', 'america', 'congress', 'government', 'years', 'american', 'nation', 'time', 'peace']\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpus_flat = [token for sentence in cleaned_corpus for token in sentence]\n",
    "\n",
    "freq_dist = nltk.FreqDist(cleaned_corpus_flat)\n",
    "print(\"Most frequent tokens:\")\n",
    "print(sorted(freq_dist, key=freq_dist.__getitem__, reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac784d",
   "metadata": {},
   "source": [
    "Looks good! But there's still a potential problem. We removed stopwords because their embeddings will be relatively meaningless. \"America\", on the other hand, is not meaningless (insert patriotic eagle call here). But \"American\" is so semantically similar to \"America\" that it might not be useful to include it as well. We can replace the words in our corpus with their more fundamental morphemes in an attempt to preserve the quality of our embeddings:\n",
    "\n",
    "e.g. American → America, government → govern, etc.\n",
    "\n",
    "There is no good way to do this, as far as I know. Many methods will do things like \"nation → n\" because \"ation\" is a common suffix. We won't do this here because the stemming involves a lot of lookups and would take forever on our dataset, but here's a sample below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4146492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stemming words: 100%|██████████| 20/20 [00:00<00:00, 20001.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['presid', 'harri', 'truman', 'address', 'joint', 'session', 'congress', 'april', '16', '1945', 'mr', 'speaker', 'mr', 'presid', 'member', 'congress', 'heavi', 'heart', 'stand', 'friend']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "stemmed_tokens = ['']*20\n",
    "for idx in tqdm.trange(len(stemmed_tokens), desc=\"Stemming words\"):\n",
    "    stemmed_tokens[idx] = stemmer.stem(cleaned_corpus_flat[idx])\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c126fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings\n",
      "Words in the vocabulary: 12311\n"
     ]
    }
   ],
   "source": [
    "# Now we train the Word2Vec model!\n",
    "from gensim.models import Word2Vec\n",
    "print(\"Creating embeddings\")\n",
    "model = Word2Vec(sentences=cleaned_corpus, vector_size=100, min_count=1, seed=0, workers=1)  # Set workers=1 to keep training deterministic\n",
    "print(f\"Words in the vocabulary: {len(model.wv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67326f3",
   "metadata": {},
   "source": [
    "Now for the obligatory NLP quote:\n",
    "## \"You shall know a word by the company it keeps.\"\n",
    "#### --John Rupert Firth\n",
    "\n",
    "As mentioned before, Word2Vec is great at generating contextual associations between words. It accomplishes this by passing a sliding window over its training samples to see which words frequently occur near other words. This forms a pretty darn good approximation of semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e73ae229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('speaker', 0.9835394620895386), ('president', 0.9781222939491272), ('members', 0.9631961584091187), ('vice', 0.9588627219200134)]\n"
     ]
    }
   ],
   "source": [
    "# We can see which words are associated with a word in the model's vocabulary\n",
    "print(model.wv.most_similar('mr', topn=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5220ec96",
   "metadata": {},
   "source": [
    "We can also retrain the model, using a \"skip-gram\" tokenization scheme instead of bag of words, and change the size of the window. This will allow us to better predict the next word in a string of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49f82078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings\n",
      "Words in the vocabulary: 12311\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating embeddings\")\n",
    "sgmodel = Word2Vec(sentences=cleaned_corpus, vector_size=100, min_count=1, seed=0, workers=1, sg=1, window=2)\n",
    "print(f\"Words in the vocabulary: {len(sgmodel.wv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba5c7db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vice', 0.9910067319869995), ('speaker', 0.9890087842941284), ('president', 0.9488757848739624), ('distinguished', 0.9383575916290283)]\n"
     ]
    }
   ],
   "source": [
    "print(sgmodel.wv.most_similar('mr', topn=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae9db0",
   "metadata": {},
   "source": [
    "## Autobots, Roll Out!\n",
    "\n",
    "Using neural networks brings us to the state of the art: transformer models. (For now, anyway. But if not: Hi, people of the future! I hope the GPT-23 is treating you well.) Transformers are great! They're inherently distributable, which helps with training on a multi-core machine, and rather than \"forgetting\" useless information like recurrent LSTM models do, they pay \"attention\" to what information is associated with other information.\n",
    "\n",
    "![attention](attention.png)\n",
    "\n",
    "This type of attention, where tokens in a sequence are associated with other tokens in the same sequence, is called self-attention. Also computed by a transformer's attention module is \"cross-attention\", which -- guess what -- associates tokens between two different sequences. \n",
    "\n",
    "![cross](cross.png)\n",
    "\n",
    "(Image from www.tensorflow.org/text/tutorials/transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53021437",
   "metadata": {},
   "source": [
    "Following is some code adapted from https://keras.io/examples/nlp/neural_machine_translation_with_transformer/ .\n",
    "\n",
    "It's not commented, because there's a lot of detail I'd have to go into in order to explain it, and I'm a busy man. Sorry.\n",
    "\n",
    "Or should I say...mi diaspiace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e415ed5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5a99e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Tom tossed Mary the ball.', '[start] Tom lanciò la palla a Mary. [end]')\n",
      "(\"I didn't have time to watch TV yesterday.\", '[start] Non avevo il tempo di guardare la TV ieri. [end]')\n",
      "('I went there recently.', '[start] Io sono andata lì recentemente. [end]')\n",
      "('What do you think of the new teacher?', '[start] Che ne pensate del nuovo professore? [end]')\n",
      "('I tried it.', \"[start] Io l'ho provato. [end]\")\n"
     ]
    }
   ],
   "source": [
    "with open(\"ita-eng/ita.txt\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, ita, junk = line.split(\"\\t\")\n",
    "    ita = \"[start] \" + ita + \" [end]\"\n",
    "    text_pairs.append((eng, ita))\n",
    "    \n",
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d47f598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364200 total pairs\n",
      "254940 training pairs\n",
      "54630 validation pairs\n",
      "54630 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_valid_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_valid_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "valid_pairs = text_pairs[num_train_samples : num_train_samples + num_valid_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_valid_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(valid_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7046f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dataset size, or else this will take forever to train\n",
    "train_pairs = text_pairs[:1000]\n",
    "valid_pairs = text_pairs[:150]\n",
    "test_pairs = text_pairs[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36e3d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length)\n",
    "ita_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization)\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_ita_texts = [pair[1] for pair in train_pairs]\n",
    "eng_vectorization.adapt(train_eng_texts)\n",
    "ita_vectorization.adapt(train_ita_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31a72524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(eng, ita):\n",
    "    eng = eng_vectorization(eng)\n",
    "    ita = ita_vectorization(ita)\n",
    "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ita[:, :-1],}, ita[:, 1:])\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, ita_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    ita_texts = list(ita_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ita_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "valid_ds = make_dataset(valid_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3ee3a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f523c78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8d97669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding[0][0]']   \n",
      " erEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, None, 15000)  12959640    ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19,960,216\n",
      "Trainable params: 19,960,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "16/16 [==============================] - 14s 680ms/step - loss: 7.4939 - accuracy: 0.2275 - val_loss: 5.8754 - val_accuracy: 0.2898\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 10s 642ms/step - loss: 5.5107 - accuracy: 0.2982 - val_loss: 5.0955 - val_accuracy: 0.3096\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 10s 652ms/step - loss: 5.0484 - accuracy: 0.3118 - val_loss: 4.7291 - val_accuracy: 0.3186\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 10s 636ms/step - loss: 4.7720 - accuracy: 0.3323 - val_loss: 4.6240 - val_accuracy: 0.3087\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 10s 643ms/step - loss: 4.5044 - accuracy: 0.3445 - val_loss: 4.2530 - val_accuracy: 0.3834\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 10s 643ms/step - loss: 4.2097 - accuracy: 0.3762 - val_loss: 4.1517 - val_accuracy: 0.3483\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 10s 632ms/step - loss: 4.0280 - accuracy: 0.3830 - val_loss: 3.6939 - val_accuracy: 0.4482\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 10s 651ms/step - loss: 3.7323 - accuracy: 0.4179 - val_loss: 3.4846 - val_accuracy: 0.4527\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 10s 644ms/step - loss: 3.5236 - accuracy: 0.4387 - val_loss: 3.2638 - val_accuracy: 0.4824\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 10s 635ms/step - loss: 3.2779 - accuracy: 0.4666 - val_loss: 2.9391 - val_accuracy: 0.5068\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 10s 649ms/step - loss: 3.0470 - accuracy: 0.4943 - val_loss: 2.9568 - val_accuracy: 0.5041\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 10s 645ms/step - loss: 2.9248 - accuracy: 0.5007 - val_loss: 2.5737 - val_accuracy: 0.5392\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 11s 687ms/step - loss: 2.6385 - accuracy: 0.5399 - val_loss: 2.3542 - val_accuracy: 0.5824\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 11s 692ms/step - loss: 2.4994 - accuracy: 0.5550 - val_loss: 2.1954 - val_accuracy: 0.5977\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 11s 675ms/step - loss: 2.2884 - accuracy: 0.5790 - val_loss: 1.9693 - val_accuracy: 0.6292\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 11s 662ms/step - loss: 2.0908 - accuracy: 0.6065 - val_loss: 1.9014 - val_accuracy: 0.6238\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 11s 668ms/step - loss: 1.9669 - accuracy: 0.6242 - val_loss: 1.7421 - val_accuracy: 0.6652\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 11s 676ms/step - loss: 1.7974 - accuracy: 0.6520 - val_loss: 1.5853 - val_accuracy: 0.6895\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 11s 684ms/step - loss: 1.7494 - accuracy: 0.6480 - val_loss: 1.3873 - val_accuracy: 0.7183\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 11s 671ms/step - loss: 1.5125 - accuracy: 0.6966 - val_loss: 1.3513 - val_accuracy: 0.7354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x155ffab4a00>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=epochs, validation_data=valid_ds, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4827372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you know that Tom was using cocaine? [start] ha detto che era troppo [end] \n",
      "\n",
      "This horse hasn't been ridden in weeks. [start] questo libro non è stato in biblioteca [end] \n",
      "\n",
      "Did you also invite your friends? [start] ha detto di non la sua sorella [end] \n",
      "\n",
      "Tom made me something to eat. [start] tom mi ha intenzione di non riesco a parlare [end] \n",
      "\n",
      "Your contribution to the school is tax-deductible. [start] la vostra di più duramente [end] \n",
      "\n",
      "This horse hasn't been ridden in weeks. [start] questo libro non è stato in biblioteca [end] \n",
      "\n",
      "Why don't you have any children yet? [start] perché non lo vuoi un errore [end] \n",
      "\n",
      "What does Tom have? [start] che i miei soldi [end] \n",
      "\n",
      "See if the gas is turned off. [start] era un dottore vero [end] \n",
      "\n",
      "Tom's parents were teachers. [start] i genitori di noi erano bagnati di noi [end] \n",
      "\n",
      "I believe there is a problem. [start] io mi sento un dottore vero [end] \n",
      "\n",
      "Sydney has a beautiful natural harbor. [start] ha un dottore vero [end] \n",
      "\n",
      "I slept soundly. [start] io sono solamente un po di noi [end] \n",
      "\n",
      "Tom says he hasn't done anything wrong. [start] tom dice di non avere fatto nulla di sbagliato [end] \n",
      "\n",
      "Say hello to your friends. [start] loro sono una buona risposta [end] \n",
      "\n",
      "You can get this medicine without a prescription. [start] devi andare al parco [end] \n",
      "\n",
      "She did not answer all the questions. [start] non ha fatto un suo libro [end] \n",
      "\n",
      "We're thorough. [start] noi siamo decisive [end] \n",
      "\n",
      "Please tell Tom to wait. [start] per favore mi ha detto di un po di tre anni [end] \n",
      "\n",
      "We liked our French teacher. [start] noi siamo a casa [end] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ita_vocab = ita_vectorization.get_vocabulary()\n",
    "ita_index_lookup = dict(zip(range(len(ita_vocab)), ita_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = ita_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = ita_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "translated_texts = []\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated = decode_sequence(input_sentence)\n",
    "    translated_texts.append(translated)\n",
    "    print(input_sentence, translated, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a17452ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] le sono un po di sua di cibo [end]\n"
     ]
    }
   ],
   "source": [
    "your_sentence = \"Am I a better translator than Google?\" # Probably not.\n",
    "tua_frase = decode_sequence(your_sentence)\n",
    "print(tua_frase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
